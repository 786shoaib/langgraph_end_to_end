{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "GOOGLE_API_KEY = os.getenv('GOOGLE_API_KEY')\n",
    "os.environ['GOOGLE_API_KEY'] = GOOGLE_API_KEY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "USER_AGENT environment variable not set, consider setting it to identify your requests.\n"
     ]
    }
   ],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain_community.vectorstores import Chroma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\langgraph_end_to_end\\genai_env\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from langchain_google_genai import GoogleGenerativeAIEmbeddings\n",
    "embedding = GoogleGenerativeAIEmbeddings(model = \"models/embedding-001\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "llm = ChatGoogleGenerativeAI(model = 'gemini-1.0-pro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GoodBye\n"
     ]
    }
   ],
   "source": [
    "while True:\n",
    "    question = input(\"Ask anything....,If you want to quit the chat write quit\")\n",
    "    if question != 'quit':\n",
    "        print(llm.invoke(question).content)\n",
    "    else:\n",
    "        print(\"GoodBye\")\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import TextLoader, DirectoryLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.vectorstores import chroma\n",
    "from langchain import PromptTemplate\n",
    "from langchain_core.runnables import RunnableParallel, RunnableLambda, RunnablePassthrough\n",
    "from langchain_core.output_parsers import StrOutputParser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = DirectoryLoader('data',glob='./*.txt',loader_cls = TextLoader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size = 30,\n",
    "    chunk_overlap = 15,\n",
    "    length_function = len\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_docs = text_splitter.split_documents(documents = docs)\n",
    "doc_strings = [doc.page_content for doc in new_docs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['LlamaIndex 3 (formerly GPT',\n",
       " '(formerly GPT Index) is a',\n",
       " 'Index) is a data framework',\n",
       " 'data framework designed to',\n",
       " 'designed to enhance how large',\n",
       " 'how large language models',\n",
       " 'models (LLMs) like Llama,',\n",
       " 'like Llama, GPT, and others',\n",
       " 'and others interact with',\n",
       " 'interact with external data',\n",
       " 'external data sources. It',\n",
       " 'sources. It enables users to',\n",
       " 'users to efficiently index,',\n",
       " 'index, retrieve, and query',\n",
       " 'and query information from',\n",
       " 'from large datasets, making',\n",
       " 'making LLMs capable of',\n",
       " 'capable of generating',\n",
       " 'of generating contextually',\n",
       " 'contextually relevant',\n",
       " 'relevant responses based on',\n",
       " 'based on data stored in',\n",
       " 'data stored in various',\n",
       " 'in various formats.',\n",
       " 'Key Features of LlamaIndex 3',\n",
       " 'Multiple Indexing Options:',\n",
       " 'Build structured indices for',\n",
       " 'indices for text documents,',\n",
       " 'documents, databases, APIs,',\n",
       " 'APIs, and more.',\n",
       " 'Supported index types include',\n",
       " 'types include tree-based',\n",
       " 'tree-based indices, vector',\n",
       " 'vector indices, and keyword',\n",
       " 'and keyword indices.',\n",
       " 'Efficient Retrieval:',\n",
       " 'Combines techniques like',\n",
       " 'like vector similarity search',\n",
       " 'search and keyword matching',\n",
       " 'matching to fetch relevant',\n",
       " 'fetch relevant data.',\n",
       " 'Integrates with vector',\n",
       " 'with vector databases like',\n",
       " 'databases like Pinecone,',\n",
       " 'like Pinecone, Weaviate, or',\n",
       " 'Weaviate, or FAISS for',\n",
       " 'or FAISS for scalable',\n",
       " 'for scalable retrieval.',\n",
       " 'Seamless LLM Integration:',\n",
       " 'Designed to work with popular',\n",
       " 'with popular LLMs like',\n",
       " \"LLMs like OpenAI's GPT\",\n",
       " \"OpenAI's GPT models, Meta's\",\n",
       " \"models, Meta's Llama, or any\",\n",
       " 'Llama, or any model supported',\n",
       " 'supported by Hugging Face.',\n",
       " 'Provides query pipelines that',\n",
       " 'pipelines that generate',\n",
       " 'that generate prompts',\n",
       " 'prompts dynamically based on',\n",
       " 'based on user queries.',\n",
       " 'Customizability:',\n",
       " 'Highly flexible for different',\n",
       " 'for different use cases, such',\n",
       " 'cases, such as knowledge',\n",
       " 'as knowledge bases, chatbots,',\n",
       " 'chatbots, search systems, and',\n",
       " 'systems, and summarization',\n",
       " 'summarization tools.',\n",
       " 'Developers can configure how',\n",
       " 'configure how data is',\n",
       " 'how data is indexed,',\n",
       " 'is indexed, retrieved, and',\n",
       " 'retrieved, and queried.',\n",
       " 'Data Augmentation:',\n",
       " 'Supports retrieval-augmented',\n",
       " 'generation (RAG) to provide',\n",
       " 'to provide up-to-date,',\n",
       " 'up-to-date, contextually',\n",
       " 'contextually relevant',\n",
       " 'relevant information to LLMs',\n",
       " 'to LLMs during generation.',\n",
       " 'Streaming and Real-Time',\n",
       " 'and Real-Time Updates:',\n",
       " 'Allows for dynamic updates to',\n",
       " 'updates to indices, ensuring',\n",
       " 'ensuring they stay current',\n",
       " 'stay current with newly added',\n",
       " 'newly added or modified data.']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "doc_strings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db = Chroma.from_documents(new_docs,embedding)\n",
    "retriever = db.as_retriever(search_kwargs = {\"k\":4})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "genai_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
